from langchain_ollama import OllamaEmbeddings, OllamaLLM
import chromadb
import os
import shutil
import pypdf  

# Define the LLM model to be used
llm_model = "llama3.1:8b"

# Configure ChromaDB and reset every run
chroma_db_path = os.path.join(os.getcwd(), "chroma_db")
if os.path.exists(chroma_db_path):
    shutil.rmtree(chroma_db_path)

# Reinitialize ChromaDB 
chroma_client = chromadb.PersistentClient(path=chroma_db_path)

class ChromaDBEmbeddingFunction:
    def __init__(self, langchain_embeddings):
        self.langchain_embeddings = langchain_embeddings

    def __call__(self, input):
        if isinstance(input, str):
            input = [input]
        embeddings = self.langchain_embeddings.embed_documents(input)
        
        if isinstance(embeddings, list) and isinstance(embeddings[0], float):
            embeddings = [embeddings]  # Convert single vector into list of lists
        
        return embeddings

# Initialize the embedding function with Ollama embeddings
embedding = ChromaDBEmbeddingFunction(
    OllamaEmbeddings(
        model=llm_model,
        base_url="http://localhost:11434"
    )
)

# Define a collection for the RAG workflow
collection_name = "rag_collection_demo"
collection = chroma_client.get_or_create_collection(
    name=collection_name,
    metadata={"description": "A collection for RAG with Ollama - Demo"},
    embedding_function=embedding  
)

def extract_text_from_pdf(pdf_path):
    reader = pypdf.PdfReader(pdf_path)
    text = ""
    for page in reader.pages:
        extracted_text = page.extract_text()
        text += extracted_text + "\n" if extracted_text else ""  
    return text.strip()

def add_pdf_to_collection(pdf_path, doc_id):
    text = extract_text_from_pdf(pdf_path)
    
    if not text:
        print(f"‚ö†Ô∏è No text extracted from {pdf_path}. Skipping.")
        return
    
    try:
        embeddings = embedding([text])
        if not embeddings:
            print(f"‚ö†Ô∏è Embeddings failed for {doc_id}. Skipping.")
            return
    except Exception as e:
        print(f"‚ùå Error generating embeddings for {doc_id}: {e}")
        return

    collection.add(documents=[text], ids=[doc_id])
    print(f"‚úÖ PDF '{doc_id}' added to ChromaDB.")
    
    all_docs = collection.get()
    print("üìå Total documents in collection:", len(all_docs["ids"]))
    print("üìù Stored document IDs:", all_docs["ids"])

def process_pdf_file(pdf_path):
    if not os.path.exists(pdf_path):
        print(f"‚ö†Ô∏è File {pdf_path} not found!")
        return

    doc_id = os.path.basename(pdf_path).replace(".pdf", "")  
    add_pdf_to_collection(pdf_path, doc_id)

# Example usage: Process all PDFs in 'data/pdf' folder
pdf_path = os.path.join(os.getcwd(), "data/pdf")
for filename in os.listdir(pdf_path):
    file_path = os.path.join(pdf_path, filename)
    if os.path.isfile(file_path) and filename.lower().endswith(".pdf"):
        process_pdf_file(file_path)

def query_chromadb(query_text, n_results=3):
    results = collection.query(
        query_texts=[query_text],
        n_results=n_results
    )
    
    print("üîñ Metadata:", results.get("metadatas", []))  
    print("üìå Retrieved IDs:", results.get("ids", [])) 
    return results.get("documents", []), results.get("metadatas", [])

def query_ollama(prompt):
    llm = OllamaLLM(model=llm_model)
    return llm.invoke(prompt)

# RAG pipeline: Combine ChromaDB and Ollama for RAG
def rag_pipeline(query_text):
    """
    Perform Retrieval-Augmented Generation (RAG) by combining ChromaDB and Ollama.
    
    Args:
        query_text (str): The input query.
    
    Returns:
        str: The generated response from Ollama augmented with retrieved context.
    """
    # Step 1: Retrieve relevant documents from ChromaDB
    retrieved_docs, metadata = query_chromadb(query_text)
    context = " ".join([" ".join(doc) for doc in retrieved_docs]) if retrieved_docs else "No relevant documents found."

    # print("######## RAG PIPELINE ########")
    # print(retrieved_docs)
    # print(metadata)

    # Step 2: Send the query along with the context to Ollama
    augmented_prompt = f"""
    You are an expert AI assistant specializing in answering user queries based on the given context or knowledge base (documents).
    Your responses should be clear, concise, and directly related to the context provided.

    Context:
    {context}

    Question:
    {query_text}

    Guidelines:
    - Answer concisely but with enough details.
    - If the context does not contain the answer, state that explicitly.
    - If necessary, provide additional insights based on general knowledge.
    - DO NOT mention where the information was retrieved from or reference specific sections of the document.

    Answer:
    """
    
    # print("######## Augmented Prompt ########")
    # print(augmented_prompt)

    response = query_ollama(augmented_prompt)
    return response

# # Example usage
# # Define a query to test the RAG pipeline
# query = "What are the factors that make us vulnerable to cyber attacks?" 
# response = rag_pipeline(query)
# print("######## Response from LLM ########\n", response)


def chatbot():
    print("üü¢ RAG Chatbot is running. Type 'exit' to quit.")
    while True:
        user_input = input("üë§ You: ")
        if user_input.lower() == "exit":
            print("üõë Chatbot session ended.")
            break
        response = rag_pipeline(user_input)
        print("ü§ñ Chatbot:", response, "\n")

def chatbot():
    print("ü§ñ Welcome! Choose an option:")
    print("1. Consult about cybersecurity risks --- You can ask anything related to cyber risks")
    print("2. Risk assessment --- Assess your risk with multiple questions and get score!")
    
    choice = input("Which one you choose (1 or 2)? ")
    
    if choice == "1":
        print("üü¢ Ask me anything about cybersecurity risks! Type 'exit' to quit.")
        while True:
            user_input = input("üë§ You: ")
            if user_input.lower() == "exit":
                print("üõë Chatbot session ended.")
                break
            response = rag_pipeline(user_input)
            print("ü§ñ Chatbot:", response, "\n")
    elif choice == "2":
        print("üìù Answer the following Yes/No questions for risk analysis.")
    else:
        print("‚ùå Invalid choice. Please restart and enter 1 or 2.")

if __name__ == "__main__":
    chatbot()
